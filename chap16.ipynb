{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75a525b2-54d5-4965-ad3f-7cbfd2e43a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7072b8dd-3b74-42fa-8d6f-8e715c0c6318",
   "metadata": {},
   "source": [
    "# Char-RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2e960d-e8fc-4b22-965f-b603e64da8da",
   "metadata": {},
   "source": [
    "## Splitting a sequence into batches of shuffled windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fa38c5e-bd1a-49d8-88ac-7a0dd923951d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ Batch 0 \n",
      "X_batch\n",
      "[[6 7 8 9]\n",
      " [2 3 4 5]\n",
      " [4 5 6 7]]\n",
      "===== \n",
      "Y_batch\n",
      "[[ 7  8  9 10]\n",
      " [ 3  4  5  6]\n",
      " [ 5  6  7  8]]\n",
      "____________________ Batch 1 \n",
      "X_batch\n",
      "[[ 0  1  2  3]\n",
      " [ 8  9 10 11]\n",
      " [10 11 12 13]]\n",
      "===== \n",
      "Y_batch\n",
      "[[ 1  2  3  4]\n",
      " [ 9 10 11 12]\n",
      " [11 12 13 14]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "n_steps=5\n",
    "dataset =tf.data.Dataset.from_tensor_slices(tf.range(15))\n",
    "dataset = dataset.window(n_steps, shift=2, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(n_steps))\n",
    "dataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[1:]))\n",
    "dataset = dataset.batch(3).prefetch(1)\n",
    "for index, (X_batch, Y_batch) in enumerate(dataset):\n",
    "    print(\"_\" * 20, \"Batch\", index, \"\\nX_batch\")\n",
    "    print(X_batch.numpy())\n",
    "    print(\"=\" *5, \"\\nY_batch\")\n",
    "    print(Y_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2d97e-257d-4fc6-8d57-215e72574d8c",
   "metadata": {},
   "source": [
    "## Loading the Data and Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e243762-5e94-4e6f-a199-2571ee158fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text =f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80e35f56-e024-4ff0-b899-62e820f7bb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc2797d-2ba8-4123-aa12-9215aa08c699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df5114a0-52be-48f6-99c6-562e7acb88d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= keras.preprocessing.text.Tokenizer(char_level =True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1602d1e-213b-4290-84ad-a2cc3e32521e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73838d53-a96a-4b2b-9507-1559aed0f5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20,6,9,8,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bc69b57-9fff-412e-8116-fba89f3334c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index)\n",
    "dataset_size = tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c1e8e07-c76e-4755-a70b-4245c3a65dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) -1\n",
    "train_size = dataset_size *90 //100\n",
    "dataset= tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc0b6f99-702e-41f4-806a-3683596d13ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps +1\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "661df4d3-a5f9-44aa-a2b7-e7009f054b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d9bfbb6-85da-40eb-bb9a-758048ec2f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84381ae2-c7d3-4e47-a652-5e11b69ebbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c213647-bb79-454e-a99c-25c209c756de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e75ddc66-852e-48f5-a6bc-d49e592316d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98776554-f938-433d-8160-f4967525bfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe277bfc-82d8-4219-97b7-decd34ece41f",
   "metadata": {},
   "source": [
    "## Creating and Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38483fc0-3902-4755-a5db-1898a6074b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "623aba0f-969f-42c4-aa8c-75966100216c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 03:56:38.617818: E tensorflow/stream_executor/cuda/cuda_dnn.cc:389] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\n",
      "2023-08-02 03:56:38.618020: E tensorflow/stream_executor/cuda/cuda_dnn.cc:398] Possibly insufficient driver version: 510.108.3\n",
      "2023-08-02 03:56:38.618037: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cudnn_rnn_ops.cc:1553 : UNKNOWN: Fail to find the dnn implementation.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential/gru/PartitionedCall]] [Op:__inference_train_function_5730]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential([keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mGRU(\u001b[38;5;241m128\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, input_shape \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m, max_id], \n\u001b[1;32m      2\u001b[0m                                                   \u001b[38;5;66;03m# dropout=0.2, recurrent_dropout=0.2,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m                                                   dropout \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m                                  keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mTimeDistributed(keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(max_id, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      8\u001b[0m                                 ])\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m history\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/.tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/venvs/.tf/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential/gru/PartitionedCall]] [Op:__inference_train_function_5730]"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([keras.layers.GRU(128, return_sequences=True, input_shape = [None, max_id], \n",
    "                                                  # dropout=0.2, recurrent_dropout=0.2,\n",
    "                                                  dropout =0.2),\n",
    "                                 keras.layers.GRU(128, return_sequences=True,\n",
    "                                                  # dropout=0.2, recurrent_dropout=0.2,\n",
    "                                                  dropout =0.2),\n",
    "                                 keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "                                ])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer =\"adam\")\n",
    "history= model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f5308c-0895-42da-bb92-e4850dbd177e",
   "metadata": {},
   "source": [
    "## handson-ml3のコードテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c109c415-ce69-4471-98bb-579f4a6b3c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer=tf.keras.layers.TextVectorization(split='character', \n",
    "                                                 standardize='lower')\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded =text_vec_layer([shakespeare_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d82d6e68-c7a8-4ce6-bbf2-9de2063844d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded-=2\n",
    "n_tokens= text_vec_layer.vocabulary_size()-2\n",
    "dataset_size=len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d839a6a-4aeb-4111-a6b0-c7e4a43dbacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79cffbe8-2a04-4402-9ba7-efadb4aeb31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "tf.random.set_seed(42)\n",
    "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True,\n",
    "                       seed=42)\n",
    "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
    "test_set = to_dataset(encoded[1_060_000:], length=length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ae60805-f584-45ac-a207-440860b87273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 04:37:15.300984: E tensorflow/stream_executor/cuda/cuda_dnn.cc:389] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n",
      "2023-08-02 04:37:15.301004: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cudnn_rnn_ops.cc:1553 : UNKNOWN: Fail to find the dnn implementation.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential_2/gru_4/PartitionedCall]] [Op:__inference_train_function_14587]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnadam\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      9\u001b[0m model_ckpt \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_shakespeare_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_ckpt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/.tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/venvs/.tf/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential_2/gru_4/PartitionedCall]] [Op:__inference_train_function_14587]"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10,\n",
    "                    callbacks=[model_ckpt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39036b6a-2e1d-4be2-815f-11d23fe34dad",
   "metadata": {},
   "source": [
    "## Using the Model to Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09ca643e-5471-474c-9799-8bf360a6d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X=np.array(tokenizer.texts_to_sequences(texts))-1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9289b90f-29d2-4960-8984-640a933b0053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 04:06:15.268586: E tensorflow/stream_executor/cuda/cuda_dnn.cc:389] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n",
      "2023-08-02 04:06:15.268632: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cudnn_rnn_ops.cc:1553 : UNKNOWN: Fail to find the dnn implementation.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Exception encountered when calling layer \"gru\" (type GRU).\n\nFail to find the dnn implementation. [Op:CudnnRNN]\n\nCall arguments received by layer \"gru\" (type GRU):\n  • inputs=tf.Tensor(shape=(1, 10, 39), dtype=float32)\n  • mask=None\n  • training=False\n  • initial_state=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m X_new\u001b[38;5;241m=\u001b[39mpreprocess([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow are yo\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_new\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msequences_to_texts(Y_pred\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/venvs/.tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/venvs/.tf/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Exception encountered when calling layer \"gru\" (type GRU).\n\nFail to find the dnn implementation. [Op:CudnnRNN]\n\nCall arguments received by layer \"gru\" (type GRU):\n  • inputs=tf.Tensor(shape=(1, 10, 39), dtype=float32)\n  • mask=None\n  • training=False\n  • initial_state=None"
     ]
    }
   ],
   "source": [
    "X_new=preprocess([\"How are yo\"])\n",
    "Y_pred = np.argmax(model(X_new), axis=-1)\n",
    "tokenizer.sequences_to_texts(Y_pred+1)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3555d07-3459-4a45-986b-dc1bb2987c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "        2, 0, 0, 1, 1, 1, 0, 0, 1, 2, 0, 0, 1, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a5716e4-37c0-417a-be48-f14d2f6f3534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new=preprocess([text])\n",
    "    y_proba=model(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba)/ temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)+1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4236409-4967-41cd-abba-0b8ca58a0c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "next_char(\"How are yo\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21458dc-be09-4260-848e-6ef52f232246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680be356-dfee-450a-8512-064b7ca84b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "print(complete_text(\"t\", temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525efc10-47ae-4f43-883c-b7695f36ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complete_text(\"t\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9be030-50a0-41a3-9f5e-d41361a94cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complete_text(\"t\", temperature=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23a927c-9cbf-429c-a9cf-4c5e952c13d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stateful RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c980000-0563-4e30-9816-77195a489029",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e872c8bd-53ee-4784-a9ea-010b1e4e119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset=dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset=dataset.batch(1)\n",
    "dataset=dataset.map(lambda windows:(windows[:,:-1], windows[:, 1:]))\n",
    "dataset =dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset =dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4572848-6f3f-4124-a089-fe8002bea813",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "encoded_parts=np.array_split(encoded[:train_size], batch_size)\n",
    "datasets=[]\n",
    "for encoded_part in encoded_parts:\n",
    "    dataset =tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "    dataset=dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    datasets.append(dataset)\n",
    "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "dataset= dataset.map(lambda windows: (windows[:,:-1], windows[:,1:]))\n",
    "dataset = dataset.map(lambda X_batch, Y_batch:(tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b94fc318-3cd5-470f-b9eb-589dfbd45f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([keras.layers.GRU(128, return_sequences=True, stateful=True, \n",
    "                                                  # dropout=0.2, recurrent_dropout=0.2,\n",
    "                                                  dropout =0.2, \n",
    "                                                  batch_input_shape=[batch_size, None, max_id]),\n",
    "                                 keras.layers.GRU(128, return_sequences=True,stateful =True,\n",
    "                                                  # dropout=0.2, recurrent_dropout=0.2,\n",
    "                                                  dropout =0.2),\n",
    "                                 keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d6530b2-72ac-4572-a527-720dafb6dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "013f5351-a315-44b2-b1b5-c9156f7f8104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 04:17:53.453958: E tensorflow/stream_executor/cuda/cuda_dnn.cc:389] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n",
      "2023-08-02 04:17:53.453985: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cudnn_rnn_ops.cc:1553 : UNKNOWN: Fail to find the dnn implementation.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential_1/gru_2/PartitionedCall]] [Op:__inference_train_function_10826]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m history\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mResetStatesCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/.tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/venvs/.tf/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential_1/gru_2/PartitionedCall]] [Op:__inference_train_function_10826]"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer =\"adam\")\n",
    "history= model.fit(dataset, epochs=50, \n",
    "                  callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9371c19e-54d7-4c99-a45c-66c77c3592a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape = [None, max_id] ),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e665df-fbc3-470b-bba4-ad589248140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.build(tf.TensorShape([None, None, max_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3732b963-f9ef-4a3e-a9e6-481af1f2da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.set_weights(model.get_weights())\n",
    "model = stateless_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe524f-f5f4-4275-a1cd-67c1a9661dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "print(complete_text(\"t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c66b1-ef86-44e7-b5f3-81872fdab61c",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd88d2c0-3ae3-45ea-b5ca-d66426b76edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86b1c26-0f5c-4c8d-b5c1-bcd009e4d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test)= keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f3cae-69d2-4abd-a4d9-3b9d4677dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53538ac6-d4f2-4e9a-919e-3b6a5a8c8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index=keras.datasets.imdb.get_word_index()\n",
    "id_to_word={id_+3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b97dc1-e6fa-4707-be20-46aeb5419b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d70dcf2-a26d-4924-8e51-5bca6b9cfd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1914dd1-2ef0-4194-bd8f-4de9360a99fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size= info.splits[\"train\"].num_examples\n",
    "test_size =info.splits[\"test\"].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d7fb2-2016-4a9b-bf6a-eab4f020d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e49f88d-7fb0-4882-8f94-2b50547009d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_batch, y_batch in datasets[\"train\"].batch(2).take(1):\n",
    "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
    "        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
    "        print(\"Label:\", label, \"=Positive\" if label else \"=Negative\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407ac36-5d7f-42be-94ca-80d9cee3cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14db7021-d01b-4aaf-a4e3-10b32b99b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76dccb6-9f20-47b1-94cc-c4ff524f4a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5389d4-d509-4ee6-9981-ad41f0254190",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e91bb1-dd2c-46db-ab83-368572496094",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65850869-659b-42c8-a2fb-e3bb87d3397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= 10000\n",
    "truncated_vocabulary=[word for word, count in vocabulary.most_common()[:vocab_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c233d567-994e-4c9b-9dfa-be596e6a6eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id= {word: index for index, word in enumerate(truncated_vocabulary)}\n",
    "for word in b\"This movie was faaaaaantastic\".split():\n",
    "    print(word_to_id.get(word) or vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343b17cd-b063-4de8-8781-e58202f46fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids=tf.range(len(truncated_vocabulary),dtype = tf.int64)\n",
    "vocab_init= tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets =1000\n",
    "table= tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa720a4-9363-42c0-8a2a-a6847eaeabb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a82e1a-b9c2-4e63-8879-c9962f7b43af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5511b65-0ede-4ff7-8cfe-8c6698e16b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_batch, y_batch in train_ste.take(1):\n",
    "    print(X_batch)\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5f6b53-10a3-453e-bc0d-82139c2fe45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, \n",
    "                                                        mask_zero = True,\n",
    "                                                        input_shape =[None]),\n",
    "                                keras.layers.GRU(128, return_sequences= True), \n",
    "                                keras.layers.GRU(128), \n",
    "                                keras.layers.Dense(1, activation = \"sigmoid\")])\n",
    "model.compile(loss= \"binary_crossentropy\", optimizer = \"adam\", metrics= [\"accuracy\"])\n",
    "history= model.fit(train_set, epochs= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b5bf36-9e7d-4d78-84a8-f6f18c73b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "K =keras.backend\n",
    "embed_size = 128\n",
    "inputs = keras.layers.Input(shape= [None])\n",
    "mask =keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
    "z= keras.layers.GRU(128, return_sequences= True)(z, mask = mask)\n",
    "z = keras.layers.GRU(128)(z, mask =mask)\n",
    "outputs = keras.layers.Dense(1, activation = \"sigmoid\")(z)\n",
    "model = keras.model.Model(inputs=[inputs], outputs= [outputs])\n",
    "model.compile(loss= \"binary_crossentropy\", optimizer = \"adam\", metrics= [\"accuracy\"])\n",
    "history= model.fit(train_set, epochs= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09358a51-4fc0-4b56-9386-edba50111c68",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa2a928-9e43-4e8a-ba31-99eacbfa5839",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff7aa3-10bd-4713-99a5-71e410159683",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFHUB_CACHE_DIR = os.path.join(os.curdir, \"my_tfhub_cache\")\n",
    "os.environ[\"TFHUB_CACHE_DIR\"]=TFHUB_CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362a0fd5-2c8e-40cc-81ca-5b6002b827dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "model = keras.Seuential([hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\", \n",
    "                                       dtype =tf.string, input_shape=[], output_shape=[50]),\n",
    "                        keras.layers.Dense(128, activation=\"relu\"),\n",
    "                        keras.layers.Dense(1, activation=\"sigmoid\")])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer =\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f876d38b-dfd3-406c-919b-bb1858f3da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirpath, dirnames, filenames in os.walk(TFHUB_CACHE_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirpath, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4950d88c-26d5-42ec-a223-68c63d21d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info =tfds.load(\"imdb_reviews\", as_supervised=True, with_info= True)\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "batch_size= 32\n",
    "train_set= datasets[\"train\"].batch(batch_size).prefetch(1)\n",
    "history=model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2f30be-cbde-4d11-8f37-f52acd4b7e08",
   "metadata": {},
   "source": [
    "## Automatic Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48717ed3-803a-4797-9ea4-3bb06740a544",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394d1d5f-39ea-44e5-a80c-a02b47468af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= 100\n",
    "embed_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c60671-3c70-460c-93f6-cfece1750d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "embeddings= keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings= embeddings(encoder_inputs)\n",
    "decoder_embeddings= embeddings(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(512, return_state= True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state= [state_h, state_c]\n",
    "\n",
    "sampler= tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell= keras.layers.LSTMCell(512)\n",
    "output_layer= keras.layers.Dense(vocab_size)\n",
    "decoder= tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, output_layer= output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths= decoder(\n",
    "    decoder_embeddings, initial_state= encoder_state, \n",
    "    sequence_length= sequence_lengths)\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model= kers.models.Model(inputs=[encoder_inputs, decoder_inputs, seuence_length], outputs= [Y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1d91a3-4a93-49dd-a7b5-68552743f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= \"sparse_categorical_crossentropy\", optimizer= \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c061973-351e-42b8-b1d1-e35207e12641",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.random.randint(100, size= 10*1000).reshape(1000, 10)\n",
    "Y=np.random.randint(100, size= 15*1000).reshape(1000, 15)\n",
    "X_decoder = np.c_[np.zeros((1000,1)), Y[:,:-1]]\n",
    "seq_lengths = np.full([1000],15)\n",
    "\n",
    "history= model.fit([X, X_decoder, seq_lengths], Y, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c43920d-e5c0-4a5d-8852-d180c7fa1cea",
   "metadata": {},
   "source": [
    "### Bidirectional Recurrent Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa15e6-0440-400a-a0ad-5bf2db8a197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(10, return_sequences=True, input_shape = [None, 10]),\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences= True))])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680eed8-b301-4ea9-805e-b4e95948e4bd",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d086f394-89f4-419a-a9df-3c5652da3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims %2 == 1: max_dims +=1\n",
    "        p, i=  np.meshgrid(np.arange(max_steps), np.arange(max_dims //2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :,::2] = np.sin(p / 10000 ** (2 *i / max_dims)).T\n",
    "        pos_emb[0, :,1::2] = np.cos(p / 10000 ** (2 *i / max_dims)).T\n",
    "        self.positional_embedding = tf.constant(pos.emb.astype(self.dtype))\n",
    "    def call(self, inputs):\n",
    "        shape= tf.shape(inputs)\n",
    "        return inputs+ self.positional_embedding[:, :shape[-2], :shape[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8864f-9fdb-444f-85d3-af5f972938b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps=201\n",
    "max_dims =512\n",
    "pos_emb= PositionalEncoding(max_steps, max_dims)\n",
    "PE=pos_emb(np.zeros((1, max_steps, max_dims), np.float32))[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7da76b-41e4-4cee-9138-c55c1139cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i1, i2, crop_i = 100, 101, 150\n",
    "p1, p2, p3 = 22, 60, 35\n",
    "fig, (ax1,ax2) = plt.subplots(nrows=2, ncols=1, sharex= True, figsize= (9,5))\n",
    "ax1.plot([p1, p1], [-1,1], \"k--\", label = \"$p = {}$\".format(p1))\n",
    "ax1.plot([p2, p2], [-1,1], \"k--\", label = \"$p = {}$\".format(p2), alpha =0.5)\n",
    "ax1.plot(p3, PE[p3, i1],, \"bx\", label = \"$p = {}$\".format(p3))\n",
    "ax1.plot(PE[:, i1], \"b-\", label = \"$i = {}$\".format(i1))\n",
    "ax1.plot(PE[:, i2], \"r-\", label = \"$i = {}$\".format(i2))\n",
    "ax1.plot([p1, p2],[PE[p1,i1], PE[p2, i1]], \"bo\")\n",
    "ax1.plot([p1, p2],[PE[p1,i2], PE[p2, i2]], \"bo\")\n",
    "ax1.legend(loc =\"center right\", fontsize =14, framealpha= 0.95)\n",
    "ax1.set_ylabel= (\"$P_{(p,i)}$\", rotation=0, fontsize= 16)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.hlines(0, 0, max_steps-1, color=\"k\", linewidth = 1, alpha =0.3)\n",
    "ax1.axis([0, max_steps-1, -1, 1])\n",
    "ax2.imshow(PE.T[:crop_i], cmap=\"gray\", interpolation= \"bilinear\", aspect=\"auto\")\n",
    "ax2.hlines(i1, 0, max_steps-1, color=\"r\")\n",
    "ax2.plot([p1, p1],[0, crop_i], \"k--\")\n",
    "ax2.plot([p2, p2],[0, crop_i], \"k--\", alpha=0.5)\n",
    "ax2.plot([p1, p2],[i2+cheat, i2+cheat], \"ro\")\n",
    "ax2.plot([p1, p2],[i1, i1], \"bo\")\n",
    "ax2.axis([0, max_steps-1, 0, crop_i])\n",
    "ax2.set_xlabel('$p$', fontsize=16)\n",
    "ax2.set_ylabel('$i$', rotation=0, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd4c0a5-7b77-4638-8046-9e408325f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size=512; max_steps=500; vocab_size=10000\n",
    "encoder_inputs= keras.layers.Input(shape=[None], dtype = np.int32)\n",
    "decoder_inputs= keras.layers.Input(shape=[None], dtype = np.int32)\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings= embeddings(encoder_inputs)\n",
    "decoer_embeddings=embeddings(decoder_inputs)\n",
    "positional_encoding = PositionalEncoding(max_steps, max_dims = embed_size)\n",
    "encoder_in = positional_encoding(encoder_embeddings)\n",
    "decoder_in = positional_encoding(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ac33e0-cdf1-4501-955b-9c5bb615e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = encoder_in\n",
    "for N in range(6):\n",
    "    Z= keras.layers.Attention(use_scale=True)([Z, Z])\n",
    "encoder_outputs= Z\n",
    "Z= decoder_in\n",
    "for N in range(6):\n",
    "    Z= keras.layers.Attention(use_scale=True, causal=True)([Z,Z])\n",
    "    Z= keras.layers.Attention(use_scale=True)([Z, encoder_outputs])\n",
    "    \n",
    "outputs=keras.layers.TimeDistributed(keras.layers.Dense(vocab_size, activation=\"softmax\"))(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2343725c-b31c-415f-bf57-2f8f7e470bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class MultiHeadAttention(keras.layers.Layer):\n",
    "    def __init__(self, n_heads, causal = False, use_scale=False, **kwargs):\n",
    "        self.n_heads= n_heads\n",
    "        self.causal= causal\n",
    "        self.use_scale =use_scale\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def build(self, batch_input_shape):\n",
    "        self.dims= batch_input_shape[0][-1]\n",
    "        self.q_dims, self.v_dims, self.k_dims = [self.dims//self.n_heads] *3\n",
    "        self.q_linear = keras.layers.Conv1D(self.n_heads * self.q_dims, kernel_size= 1, use_bias= False)\n",
    "        self.v_linear = keras.layers.Conv1D(self.n_heads * self.v_dims, kernel_size= 1, use_bias= False)\n",
    "        self.k_linear = keras.layers.Conv1D(self.n_heads * self.k_dims, kernel_size= 1, use_bias= False)\n",
    "        self.attention = keras.layers.Attention(causal= self.causal, use_scale= self.use_scale)\n",
    "        self.out_linear= keras.layers.Conv1D(self.dims, kernel_size=1, use_bias= False)\n",
    "        super().build(batch_input_shape)\n",
    "    \n",
    "    def _multi_head_linear(self, inputs, linear):\n",
    "        shape= K.concatenate([K.shape(inputs)[:-1], [self.n_heads, -1]])\n",
    "        projected= K.reshape(linear(inputs), shape)\n",
    "        perm = K.permute_dimensions(projected, [0, 2, 1,3])\n",
    "        return K.reshape(perm, [shape[0]*self.n_heads, shape[1], -1])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        q = inputs[0]\n",
    "        v= inputs[1]\n",
    "        k = inputs[2] if len(inputs) > 2 else v\n",
    "        shape = K.shape(q)\n",
    "        q_proj = self._multi_head_linear(q, self.q_linear)\n",
    "        v_proj = self._multi_head_linear(v, self.v_linear)\n",
    "        k_proj = self._multi_head_linear(k, self.k_linear)\n",
    "        multi_attended = self.attention([q_proj, v_proj, k_proj])\n",
    "        shape_attended = K.shape(multi_attended)\n",
    "        reshaped_attended= K.reshape(multi_attended, [shape[0], self.n_heads, shape_attended[1], shape_attended[2]])\n",
    "        perm= K.permute_dimensions(reshaped_attended, [0, 2, 1,3])\n",
    "        concat = K.reshape(perm, [shape[0], shape_attended[1],-1])\n",
    "        return self.out_linear(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a21a6ac-498a-424b-8f98-060a7899043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.random.rand(2, 50, 512)\n",
    "V= np.random.rand(2, 80, 512)\n",
    "multi_attn= MultiHeadAttention(8)\n",
    "multi_attn([Q, V]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_tf",
   "language": "python",
   "name": "ml_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
