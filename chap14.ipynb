{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9d60a1b-2392-4174-8d64-16b97d4d31e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sklearn\u001b[38;5;241m.\u001b[39m__version__ \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.20\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__version__ \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >=\"2.0\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943bc2de-3580-446c-8228-6b3135274a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "    plt.imshow(image, cmap=\"gray\", interpolation='nearest')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "def plot_color_image(image):\n",
    "    plt.imshow(image, interpolation='nearest')\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba23bf7-81e0-4569-a6a7-bbc83034cac6",
   "metadata": {},
   "source": [
    "# What is a Convolution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffef755-da13-48e8-a9a6-93f01dd5291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_image\n",
    "\n",
    "china=load_sample_image(\"china.jpg\")/255\n",
    "flower=load_sample_image(\"flower.jpg\")/255\n",
    "images=np.array([china, flower])\n",
    "batch_size, height, width, channels = images.shape\n",
    "\n",
    "filters=np.zeros(shape=(7,7,channels, 2),dtype=np.float32)\n",
    "filters[:,3,:,0] =1 # vertical line\n",
    "filters[3,:,:,1] =1 # horizontal line\n",
    "\n",
    "outputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")\n",
    "\n",
    "plt.imshow(outputs[0,:,:,1], cmap='gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e2d1f-cfb2-425e-9471-813d5cffa885",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_index in (0,1):\n",
    "    for feature_map_index in (0,1):\n",
    "        plt.subplot(2,2, image_index*2 + feature_map_index+1)\n",
    "        plot_image(outputs[image_index,:,:,feature_map_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f45d06-1f06-402b-bd5b-9f4c9b855605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(image):\n",
    "    return images[150:220, 130:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a101f04d-3c3c-47b2-b019-8ca0a8c2a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(crop(images[0,:,:,0]))\n",
    "\n",
    "for feature_map_index, filename in enumerate([\"china_vertical\", \"china_horizontal\"]):\n",
    "    plot_image(crop(outputs[0,:,:,feature_map_index]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebce159-9c17-4bd6-b06c-b85e6e0fc23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(filters[:,:,0,0])\n",
    "plot_image(filters[:,:,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3643c86d-896e-49da-b9b3-8040c621216c",
   "metadata": {},
   "source": [
    "## Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f57df0-4880-44b5-93a4-0ad33c014975",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = keras.layers.Conv2D(filters =2, kernel_size=7, strides=1,\n",
    "                          padding=\"SAME\", activation='relu', input_shape=outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9b5ff2-e27a-4480-9f2b-62c00a40ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_outputs=conv(images)\n",
    "conv_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dfb9d6-b380-402a-8bdc-c29cda88f91a",
   "metadata": {},
   "source": [
    "次元はバッチサイズ、高さ、幅、チャンネル。  \n",
    "この畳込み層は2つのフィルターを持つのでチャンネルの次元数は2になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73317e43-31b7-41b6-a9f7-54328b57e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10,6))\n",
    "for image_index in (0,1):\n",
    "    for feature_map_index in (0,1):\n",
    "        plt.subplot(2,2, image_index*2+feature_map_index+1 )\n",
    "        plot_image(crop(conv_outputs[image_index, :,:,feature_map_index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e05698-c8db-401c-9d74-ca212f93425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.set_weights([filters, np.zeros(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c62a9d-1c96-40ba-8055-f976a7150324",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_outputs=conv(images)\n",
    "conv_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88946736-7f3d-4e32-9483-ceb8f2dfe175",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10,6))\n",
    "for image_index in (0,1):\n",
    "    for feature_map_index in (0,1):\n",
    "        plt.subplot(2,2, image_index*2+feature_map_index+1 )\n",
    "        plot_image(crop(conv_outputs[image_index, :,:,feature_map_index]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb27d56a-b544-4e6c-8012-c57395ece317",
   "metadata": {},
   "source": [
    "## VALID vs SAME padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b919165d-66bc-415e-8371-16335fc0bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_map_size(input_size, kernel_size, strides=1, padding=\"SAME\"):\n",
    "    if padding=='SAME':\n",
    "        return (input_size-1)//strides +1\n",
    "    else:\n",
    "        return (input_size -kernel_size)//strides+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece27e9e-1682-4474-9e6c-8b08ff2694f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_before_and_padded_size(input_size, kernel_size, strides=1):\n",
    "    fmap_size=feature_map_size(input_size, kernel_size, strides)\n",
    "    padded_size=max((fmap_size -1)*strides + kernel_size, input_size)\n",
    "    pad_before=(padded_size-input_size)//2\n",
    "    return pad_before, padded_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacd10b7-bb10-4780-a726-2553faf9815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_same_padding(images, kernel_size, strides=1):\n",
    "    if kernel_size==1:\n",
    "        return images.astype(np.float32)\n",
    "    batch_size, height, width, channels=images.shape\n",
    "    top_pad, padded_height=pad_before_and_padded_size(height, kernel_size, strides)\n",
    "    left_pad, padded_widtht=pad_before_and_padded_size(width, kernel_size, strides)\n",
    "    padded_shape  = [batch_size, padded_height, padded_width, channels]\n",
    "    padded_images=np.zeros(padded_shape, dtype=np.float32)\n",
    "    padded_images[:, top_pad:hieght+top_pad, left_pad:width+left_pad, :]=images\n",
    "    return padded_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16761e-168a-4627-b10b-f6607526562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size =7\n",
    "strides =2\n",
    "\n",
    "conv_valid = keras.layers.Conv2D(filters=1, kernel_size=kernel_size, strides=strides, padding=\"VALID\")\n",
    "conv_same = keras.layers.Conv2D(filters=1, kernel_size=kernel_size, strides=strides, padding=\"SAME\")\n",
    "\n",
    "valid_output = conv_valid(manual_same_padding(images, kernel_size, strides))\n",
    "\n",
    "conv_same.build(tf.TensorShape(images.shape))\n",
    "\n",
    "conv_same.set_weights(conv_valid.get_weights())\n",
    "\n",
    "same_output = conv_same(images.astype(np.float32))\n",
    "\n",
    "assert np.allclose(valid_output.numpy(), same_output.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b8c944-0a60-4934-9bdb-ff0e108a7b02",
   "metadata": {},
   "source": [
    "# Pooling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f00d1b-826f-4bf2-9d95-31e22401ed89",
   "metadata": {},
   "source": [
    "## Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3756ed2-60ad-43e6-9079-9067db0ac4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool=keras.layers.MaxPool2D(pool_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39154e95-6319-4792-92e3-2cddc828d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images= np.array([crop(image) for image in images], dtype = np.float32)\n",
    "output= max_pool(cropped_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaab515-b181-4ecd-8545-481007c6114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize =(12,8))\n",
    "gs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[2,1])\n",
    "\n",
    "ax1= fig.add_subplot(gs[0,0])\n",
    "ax1.set_title('Input', fontsize =14)\n",
    "ax1.imshow(cropped_images[0])\n",
    "ax1.axis('off')\n",
    "ax2= fig.add_subplot(gs[0,1])\n",
    "ax2.set_title('Output', fontsize =14)\n",
    "ax2.imshow(output[0])\n",
    "ax2.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5747cab-6632-4859-8622-cd7f3a6813a7",
   "metadata": {},
   "source": [
    "## Depth-wise pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c304165b-47b9-4c58-ab49-670763a062d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDepthMaxPool\u001b[39;00m(\u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLayer):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pool_size, strdies\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVALID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "class DepthMaxPool(keras.layers.Layer):\n",
    "    def __init__(self, pool_size, strdies=None, padding=\"VALID\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if strides is None:\n",
    "            strides = pool_size\n",
    "        self.pool_size = pool_size\n",
    "        self.strides= strides\n",
    "        self.padding = padding\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.max_pool(inputs,\n",
    "                             ksize = (1,1,1, self.pool_size),\n",
    "                             strides = (1, 1, 1, self.pool_size),\n",
    "                             padding = self.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa6ee8-14e4-45d6-8819-caedb432c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_pool = DepthMaxPool(3)\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    depth_output = depth_pool(cropped_images)\n",
    "depth_output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45c86a2-6fff-4841-a158-7e15e7bbf095",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_pool = keras.layers.Lambda(lambda X: tf.nn.max_pool(\n",
    "    X, ksize = (1,1,1,3), strides = (1,1,1,3), padding = 'VALID'))\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    depth_output = depth_pool(cropped_images)\n",
    "depth_output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b6e71-110c-420d-acbb-ba34c132766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (12,6))\n",
    "plt.subplot(1, 2,1)\n",
    "plt.title(\"Input\", fontsize =14)\n",
    "plot_color_image(cropped_images[0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Output\", fontsize =14)\n",
    "plot_image(depth_output[0, ..., 0])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92845c7-eed1-4555-8290-65a319658079",
   "metadata": {},
   "source": [
    "## Average pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0325d9e9-2348-4b86-88b8-4787989e106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_pool = keras.layers.AvgPool2D(pool_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb6963-6fa4-449d-8722-aa31152377e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_avg - avg_pool(cropped_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb80cd2b-9bb8-41cb-bfdc-8da214ab0b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "gs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[2,1])\n",
    "\n",
    "ax1= fig.add_subplot(gs[0,0])\n",
    "ax1.set_title('Input', fontsize =14)\n",
    "ax1.imshow(cropped_images[0])\n",
    "ax1.axis('off')\n",
    "ax2= fig.add_subplot(gs[0,1])\n",
    "ax2.set_title('Output', fontsize =14)\n",
    "ax2.imshow(output_avg[0])\n",
    "ax2.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dcd730-0bdb-4158-86d8-3e7fde00056d",
   "metadata": {},
   "source": [
    "## Global Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085bbd88-7f82-4836-bd59-038d04ca5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_avg_pool = keras.layers.GlobalAvgPool2D()\n",
    "global_avg_pool(cropped_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd18d57-d222-4f97-8097-9dd18298508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_global_avg2 = keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis=[1,2]))\n",
    "output_global_avg2(cropped_iamges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da434a47-687c-496b-9539-9412f0a4622a",
   "metadata": {},
   "source": [
    "# Tackling Fashion MNIST With a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a3c0ec-6dd6-45f6-acaa-ba27ad87e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
    "\n",
    "X_mean= X_train.mean(axis=0, keepdims= True)\n",
    "X_std =X_train.std(axis=0, keepdims= True)+ 1e-7\n",
    "X_train= (X_train-X_mean)/X_std\n",
    "X_valid= (X_valid-X_mean)/X_std\n",
    "X_test= (X_test-X_mean)/X_std\n",
    "\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_valid = X_valid[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db6f2b8-7c1a-44ba-852e-bfb414f1bfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "DefaultConv2D = partial(keras.layers.Conv2D,\n",
    "                        kernel_size=3, activation='relu', padding='SAME')\n",
    "\n",
    "model=keras.models.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7,input_shape=[28,28,1]),\n",
    "    keras.layers.MaxPooling2D(pool_size =2),\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    keras.layers.MaxPooling2D(pool_size =2),\n",
    "    DefaultConv2D(filters=256),\n",
    "    DefaultConv2D(filters=256),\n",
    "    keras.layers.MaxPooling2D(pool_size =2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=64, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a19af8-19cf-4ac9-a57f-c6e24ae04c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= 'sparse_categorical_crossentropy', optimizer = 'nadam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "score = model.evaluate(X_test, y_test)\n",
    "X_new= X_test[:10]\n",
    "y_pred=model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aae502-37e5-4f74-97bd-8bddc0e3eec9",
   "metadata": {},
   "source": [
    "## ResNet-34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7328a45-a935-4894-a26c-089070535973",
   "metadata": {},
   "outputs": [],
   "source": [
    "DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, stride=1,\n",
    "                       padding='SAME', use_bias=False)\n",
    "\n",
    "class ResidualUnit(keras.layers.Layer):\n",
    "    def __init__(self, filters, stride =1, activation =\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters, strides= strides),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            self.activation, \n",
    "            DefaultConv2D(filters),\n",
    "            keras.layers.BatchNormalization()]\n",
    "        self.skip_layers=[]\n",
    "        if strides >1 :\n",
    "            self.skip_layers =[\n",
    "                DefaultConv2D(filters, kernel_size =1, strides =strides),\n",
    "                keras.layers.BatchNormalization()]\n",
    "            \n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z=layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z=layer(skip_Z)\n",
    "        return self.activation(Z+skip_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c7ec7b-a51a-4fdd-a4e3-e5667dfd9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(DefaultConv2D(64, kernel_size=7, strides =2,\n",
    "                        input_shape=[224,224,3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(\"relu\"))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2,padding=\"SAME\"))\n",
    "prev_filters = 64\n",
    "for filters in [64 ]*3 +[128] * 4 + [256]* 6+[512]*3:\n",
    "    strides = 1 if fiters == prev_filters else 2\n",
    "    model.add(ResidualUnit(filters, strides = strides))\n",
    "    prev_filters = filters\n",
    "model.add(keras.layers.GlobalAvgPool2D())\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(10, activation = 'softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a82513-0387-442b-98de-f606fc6d36c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2917451-5fae-4ebd-b8c7-04e936a82f97",
   "metadata": {},
   "source": [
    "## Using a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034c06d-9095-4684-8379-aa9102cf5855",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.applications.resnet50.ResNet50(weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0338be-f634-4099-a6d8-94cd00bc31f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_resized=tf.image.resize(imagese, [224,224])\n",
    "plot_color_image(images_resized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d47dbc-ed27-4714-af0f-a28a402e3fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_resized=tf.image.resize_with_pad(imagese, 224,224, antialias=True)\n",
    "plot_color_image(images_resized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cad599-fddd-4e0e-b6ef-b134f7e59555",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_resized=tf.image.resize_with_crop_or_pad(imagese, 224,224)\n",
    "plot_color_image(images_resized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2de68e-5106-4d37-80d8-5dee6e84bd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "china_box=[0,0.03,1,0.68]\n",
    "flower_box=[0.19,0.26,0.86,0.7]\n",
    "images_resized=tf.images.crop_and_resize(images,[china_box,flower_box],[0,1],[224,224])\n",
    "plot_color_image(images_resized[0])\n",
    "plot_color_image(images_resized[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a786817-46ac-431b-bdf1-47fa2d2c7a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=keras.applications.resnet50.preprocess_input(images_resized*255)\n",
    "Y_proba=model.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b248f05-a4cd-4741-b2a5-fd96c23f1afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c3190c-bcaf-426d-8ab0-78bcf3300e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_K=keras.applications.resnet50.decode_predictions(Y_proba, top=3)\n",
    "for image_index in range(len(images)):\n",
    "    print(\"Image #{}\".format(image_index))\n",
    "    for class_id, name, y_proba in top_K[image_index]:\n",
    "        print(\" {} - {:12s} {:.2f}%\".format(class_id, name, y_proba*100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16007b4-9277-428c-87d5-f468f3d24810",
   "metadata": {},
   "source": [
    "## Pretrained Models for Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5f23e4-ca53-4a03-911f-17c4c7f72ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset, info =tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a2d41c-aea3-4e08-93ae-f2d19a6c2240",
   "metadata": {},
   "outputs": [],
   "source": [
    "info.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34cfdce-9675-442c-a9dd-e0b0ca6b976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "info.splits[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a38e7e-43c0-4c44-9311-05c54bcb829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=info.features[\"label\"].names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b75b59-302c-4e98-a544-aced088d8f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes =info.features[\"label\"].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b1abed-8c58-44bc-bd1a-b9c97faf6873",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = info.splits[\"train\"].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071df20d-570d-4e1f-abe6-253d47343300",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_raw, valid_set_raw, train_set_raw =tfds.load(\n",
    "    \"tf_flowers\", \n",
    "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac96cff-0251-4109-a6ff-7c24f0a76d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "index = 0\n",
    "for image, label in train_set_raw.take(9):\n",
    "    index+=1\n",
    "    plt.subplot(3, 3, index)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Class: {}\".format(class_names[label]))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d31d8cb-7e6f-4410-9644-3f3056db4661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4518c45d-c0a4-4e69-b85f-58be1efc8571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c9fc68-4dff-4996-a6ce-9d77f1c993fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
